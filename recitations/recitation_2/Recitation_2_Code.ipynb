{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# AI for Transportation | Recitation 2: Discrete Choice Modeling\n",
        "\n",
        "In this recitation, we study **Discrete Choice Modeling (DCM)** using both classical baselines and a modern, theory-informed neural approach: the **Alternative-Specific Utility Deep Neural Network (ASU-DNN)**. Our goal is to understand how utility-theoretic structure can guide neural architectures to achieve **better prediction**, **clearer interpretation**, and **stronger generalization** than vanilla fully connected models.\n",
        "\n",
        "We will work with the [1987 Netherlands **Train**](https://open.rijkswaterstaat.nl/@120703/the-netherlands-value-time-study-final/) mode-choice dataset via the R [**mlogit**](https://cran.r-project.org/web/packages/mlogit/index.html) package. You can find information on the dataset in the mlogit [documentation](https://www.rdocumentation.org/packages/mlogit/versions/0.2-4/topics/Train). Although the original survey microdata are not directly distributed, the `Train` dataset is accessible programmatically from CRAN; in Colab we will fetch it using **rpy2** and convert it to Python (pandas) for modeling.\n",
        "\n",
        "We will proceed in four parts:\n",
        "\n",
        "1. **Data Access & Preparation**  \n",
        "   - Install and load **mlogit** in an R runtime within Colab using **rpy2**.  \n",
        "   - Import the `Train` dataset and convert it to long/choice-format pandas DataFrames.  \n",
        "   - Split into train/validation/test with traveler-level grouping.\n",
        "\n",
        "2. **Classical Baselines (Utility Models)**  \n",
        "   - Fit **Multinomial Logit (MNL)** with alternative-specific constants and key attributes (time, cost, etc.).  \n",
        "   - Report **in-sample fit** and **out-of-sample accuracy** / log-likelihood.  \n",
        "   - Extract **elasticities**, **value of time (VOT)**, and **substitution patterns** for interpretability.\n",
        "\n",
        "3. **ASU-DNN (Theory-Guided Architecture)**  \n",
        "   - Build a sparse, **alternative-specific** network where each mode’s attributes feed only its own utility head (softmax over utilities).  \n",
        "   - Train with cross-entropy; compare against a **fully connected DNN** of similar capacity.  \n",
        "   - Evaluate **accuracy**, **calibration**, and **interpretability** (e.g., cost/time response curves per mode).\n",
        "\n",
        "4. **Comparison & Reflection**  \n",
        "   - Side-by-side results for **MNL vs. FC-DNN vs. ASU-DNN**.  \n",
        "   - Discuss the trade-off between **approximation power** and **estimation stability**, and how ASU-DNN acts as a **domain-knowledge regularizer**.  \n",
        "   - Optional: examine **robustness** (small cost/time perturbations) and **IIA-like behaviors** under different architectures.\n",
        "\n",
        "By the end, you will:\n",
        "- Load and prepare a **choice-format** dataset from R inside a Python notebook.  \n",
        "- Fit and interpret a **classical MNL** model.  \n",
        "- Implement and train a **theory-informed ASU-DNN**, and compare it with a **vanilla DNN**.  \n",
        "- Relate architectural choices to **economic interpretability** and **generalization** in discrete choice tasks.\n",
        "\n",
        "**References:**  \n",
        "- CRAN `mlogit` package — Train dataset documentation.  \n",
        "- Lecture materials on **ASU-DNN** and **theory-based neural architectures** for choice modeling.\n",
        "\n",
        "We will be using data from [The Netherlands' Reports](https://open.rijkswaterstaat.nl/@120703/the-netherlands-value-time-study-final/). Since the original dataset is not available in a stand-alone fashion, we are making use of the R-language CRAN package [MLogit](https://cran.r-project.org/web/packages/mlogit/index.html). You can find information on the specific `Train` (the vehicle) dataset [here](https://www.rdocumentation.org/packages/mlogit/versions/0.2-4/topics/Train)."
      ],
      "metadata": {
        "id": "h8_eDoLVhx9u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8CkKxYS1QEbj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "outputId": "33e82c60-e117-407f-dcc3-f315a700301a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'util_nn_mlarch'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1304052541.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# Custom utilities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mutil_nn_mlarch\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# Reproducibility and plotting style\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'util_nn_mlarch'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "# Imports\n",
        "# Standard library\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "# Core scientific stack\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.stats as ss\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "\n",
        "# Scikit-learn\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split, cross_validate\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC, LinearSVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "\n",
        "# Custom utilities\n",
        "import util_nn_mlarch as util\n",
        "\n",
        "# Reproducibility and plotting style\n",
        "RANDOM_STATE = 100\n",
        "np.random.seed(RANDOM_STATE)\n",
        "sns.set(context=\"notebook\", style=\"whitegrid\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We support either:\n",
        "# 1) mlogit_Train_wide.csv (simplest), or\n",
        "# 2) mlogit_choice_data.pickle with a 'Train_wide' key.\n",
        "\n",
        "def load_train_wide(uploaded_names):\n",
        "    if any(name.endswith(\"mlogit_Train_wide.csv\") for name in uploaded_names):\n",
        "        csv_name = [n for n in uploaded_names if n.endswith(\"mlogit_Train_wide.csv\")][0]\n",
        "        df = pd.read_csv(csv_name)\n",
        "        source = \"csv\"\n",
        "    elif any(name.endswith(\"mlogit_choice_data.pickle\") for name in uploaded_names):\n",
        "        pkl_name = [n for n in uploaded_names if n.endswith(\"mlogit_choice_data.pickle\")][0]\n",
        "        import pickle\n",
        "        with open(pkl_name, \"rb\") as f:\n",
        "            data_dic = pickle.load(f)\n",
        "        if \"Train_wide\" not in data_dic:\n",
        "            raise KeyError(\"Train_wide not found in the pickle. Available keys: \"\n",
        "                           + \", \".join(data_dic.keys()))\n",
        "        df = data_dic[\"Train_wide\"].copy()\n",
        "        source = \"pickle\"\n",
        "    else:\n",
        "        raise FileNotFoundError(\"Please upload mlogit_Train_wide.csv or mlogit_choice_data.pickle\")\n",
        "    return df, source\n",
        "\n",
        "df_raw, source = load_train_wide(uploaded_names)\n",
        "print(f\"Loaded Train dataset from {source}. Shape:\", df_raw.shape)\n",
        "display(df_raw.head(3))"
      ],
      "metadata": {
        "id": "MTWChuF1Stdd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I_N7Tir7SwoI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Loading"
      ],
      "metadata": {
        "id": "lQbE8WkfqTPa"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HaAwxBfLqUq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Baseline Model Definitions"
      ],
      "metadata": {
        "id": "wTyhSJXLqVH2"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gcBAMyW6qX3g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ASU-DNN Model Definition"
      ],
      "metadata": {
        "id": "twWx6n6QqYVE"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wnQsQmIBqbQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Benchmark Inference"
      ],
      "metadata": {
        "id": "tXGxR9igqbtT"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KFqwjZIsqfmb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ASU-DNN Inference"
      ],
      "metadata": {
        "id": "J74-3l4vqe0p"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NrnLsHwtqhmr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analysis"
      ],
      "metadata": {
        "id": "5vwKTuNHqhx7"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Cl4vDx00qlBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results and Insights"
      ],
      "metadata": {
        "id": "JQj_2uMBqkRX"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "resGMct1qmvY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Where to Go from Here?"
      ],
      "metadata": {
        "id": "knpW72jfqnNu"
      }
    }
  ]
}